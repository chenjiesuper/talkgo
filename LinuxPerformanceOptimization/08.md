## iowait 分析
1. dstat 观察 CPU 和 I/O 的使用情况
```
# 间隔1秒输出10组数据
$ dstat 1 10
```
当 iowait 升高（wai）时，磁盘的读请求（read）都会很大。这说明 iowait 的升高跟磁盘的读请求有关，很可能就是磁盘读导致的
2. top找到D状态pid
3. pidstat -d 查看pid的 I/O 统计数据 没发现明显问题
```
# -d 展示 I/O 统计数据，-p 指定进程号，间隔 1 秒输出 3 组数据
$ pidstat -d -p 4344 1 3
```
4. pidstat -d 查看全部进程的I/O 统计数据，发现进程想要访问磁盘，就必须使用系统调用，接下来，重点就是找出 app 进程的系统调用
```
# 间隔 1 秒输出多组数据 (这里是 20 组)
$ pidstat -d 1 20
```
5. strace -p 进程失败
6. ps aux 查进程 发现已经僵尸无法strace
7. 此时top,pidstat不能给出更多信息
8. perf record -g , perf report展开调用栈结合源码发现直接访问磁盘导致

## 僵尸进程分析
找出父进程，然后在父进程里解决。
1. pstree
```
# -a 表示输出命令行选项
# p表PID
# s表示指定进程的父进程
$ pstree -aps 3084
systemd,1
  └─dockerd,15006 -H fd://
    └─docker-containe,15024 --config /var/run/docker/containerd/containerd.toml
      └─docker-containe,3991 -namespace moby -workdir...
        └─app,4009
          └─(app,3084)
```

## 小结
 **iowait 高不一定代表 I/O 有性能瓶颈。当系统中只有 I/O 类型的进程在运行时，iowait 也会很高，但实际上，磁盘的读写远没有达到性能瓶颈的程度。**
因此，碰到 iowait 升高时，需要先用 dstat、pidstat 等工具，确认是不是磁盘 I/O 的问题，然后再找是哪些进程导致了 I/O。
等待 I/O 的进程一般是不可中断状态，所以用 ps 命令找到的 D 状态（即不可中断状态）的进程，多为可疑进程。但这个案例中，在 I/O 操作后，进程又变成了僵尸进程，所以不能用 strace 直接分析这个进程的系统调用。
这种情况下，我们用了 perf 工具，来分析系统的 CPU 时钟事件，最终发现是直接 I/O 导致的问题。这时，再检查源码中对应位置的问题，就很轻松了。
而僵尸进程的问题相对容易排查，使用 pstree 找出父进程后，去查看父进程的代码，检查 wait() / waitpid() 的调用，或是 SIGCHLD 信号处理函数的注册就行了。
